#!/usr/bin/env python3
"""
Training script for Prunnerai 3B V.1.0
Generated by Priority Living Labs Model Forge

Framework:  pytorch
Algorithms: lora
"""

import json
import os
import argparse

CONFIG_PATH = os.path.join(os.path.dirname(__file__), "..", "models", "Prunnerai 3B V.1.0", "config.json")

def load_config():
    with open(CONFIG_PATH) as f:
        return json.load(f)


import torch
import torch.nn as nn
from torch.utils.data import DataLoader


def train(config):
    from src.model import build_model
    from src.dataset import load_dataset

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = build_model().to(device)

    dataset = load_dataset("data/")
    loader = DataLoader(dataset, batch_size=4, shuffle=True)

    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(loader) * 3)
    scaler = torch.amp.GradScaler("cuda")

    grad_accum_steps = 8
    model.train()

    for epoch in range(3):
        total_loss = 0
        optimizer.zero_grad()

        for step, batch in enumerate(loader):
            tokens = batch["input_ids"].to(device)
            labels = tokens.clone()

            with torch.amp.autocast("cuda"):
                logits = model(tokens)
                loss = nn.functional.cross_entropy(
                    logits[:, :-1, :].contiguous().view(-1, logits.size(-1)),
                    labels[:, 1:].contiguous().view(-1),
                )
                loss = loss / grad_accum_steps

            scaler.scale(loss).backward()
            total_loss += loss.item()

            if (step + 1) % grad_accum_steps == 0:
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                scaler.step(optimizer)
                scaler.update()
                scheduler.step()
                optimizer.zero_grad()

            if step % 100 == 0:
                print(f"Epoch {epoch+1} | Step {step} | Loss {loss.item() * grad_accum_steps:.4f}")

        avg_loss = total_loss / max(len(loader), 1)
        print(f"Epoch {epoch+1} complete — avg loss: {avg_loss:.4f}")
        torch.save(model.state_dict(), f"checkpoints/epoch_{epoch+1}.pt")

    torch.save(model.state_dict(), "checkpoints/final.pt")
    print("✅ Training complete — checkpoints saved to checkpoints/")


if __name__ == "__main__":
    config = load_config()
    train(config)
